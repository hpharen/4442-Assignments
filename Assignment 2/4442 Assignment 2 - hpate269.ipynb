{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. SVM and Margin\n",
    "Figure 1 shows a set of data points and a decision boundary returned by SVM. For each data point (points 1, 2, 3, 4, 5), please answer:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### 1) If the decision boundary will be changed if the data point is removed.\n",
    "\n",
    "Point 1: No, the decision boundary $\\textbf{will not}$ change if the data point is removed.\n",
    "\n",
    "Point 2: Yes, the decision boundary $\\textbf{will}$ change if the data point is removed.\n",
    "\n",
    "Point 3: Yes, the decision boundary $\\textbf{will}$ change if the data point is removed.\n",
    "\n",
    "Point 4: No, the decision boundary $\\textbf{will not}$ change if the data point is removed.\n",
    "\n",
    "Point 5: No, the decision boundary $\\textbf{will not}$ change if the data point is removed."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### 2) Explain your answer with at most 2 sentence (for each data point).\n",
    "\n",
    "Point 1: This point is a not a support vector, as it is inside of the decision boundary. Removing it will not change the position of the decision boundary.\n",
    "\n",
    "Point 2: This point is also a support vector, removing it will alter the margin and decision boundary.\n",
    "\n",
    "Point 3: Since this point lies on the margin and is a support vector, removing it will cause the decision boundary to adjust.\n",
    "\n",
    "Point 4: This point is not a support vecto and does not contribute to defining the margin. Removing it will not affect the decision boundary.\n",
    "\n",
    "Point 5: This point is far from the decision boundary and does not influence it. Removing it will not change the boundary."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. Kernels\n",
    "In this problem, we consider constructing new kernels by combining existing kernels. Recall that for some function $k(x, z)$ to be a kernel, we need to be able to write it as a dot product of vectors in some high-dimensional feature space defined by $\\phi$:\n",
    "$$\n",
    "k(x, z) = \\phi(x)^\\top \\phi(z)\n",
    "$$\n",
    "Mercer’s theorem gives a necessary and sufficient condition for a function $k$ to be a kernel function: its corresponding kernel matrix $K$ has to be symmetric and positive semidefinite. Suppose that $k_1(x, z)$ and $k_2(x, z)$ are two valid kernels. For each of the cases below, state whether k is also a valid kernel. If it is, prove it. If it is not, give a counterexample. You can use either Mercer’s theorem, or the definition of a kernel as needed to prove it (If you use any properties on page 10 of Lecture 8, we need to prove them first)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "(a) $k(x, z) = a_1k_1(x, z) − a_2k_2(x, z)$, where $a_1, a_2 > 0$ are real numbers"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "(b) $k(x, z) = \\sqrt{k_1(x, z)k_2(x, z)}$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "(c) If $k(x, z) = e^{\\frac{x^\\top z}{\\sigma^2}}$ is a valid kernel, prove that the Gaussian kernel $k(x, z) = e^{-\\frac{\\|x - z\\|^2_2}{2\\sigma^2}}$ is also a valid kernel.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3. PCA and Eigenface\n",
    "For this exercise, you will use principal component analysis (PCA) to analyze face images in any programming language of your choice (e.g., Python/Matlab/R). The data set faces.dat; each row represents an image (400 images), and each column represents a pixel (64 × 64 = 4096 pixels)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "(a) Display the 200th image."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "(b) Remove the mean of the images, and then display the 100th image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "(c) Perform PCA on the mean-centered data matrix. You can either implement PCA by yourself using eigenvalue decomposition over the sample covariance matrix, or use a existing\n",
    "machine learning toolbox. Sort the eigenvalues in a descending order and plot them."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "(d) You will find the last (i.e., 400th) eigenvalue is 0. Explain why"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "(e) Based on the eigenvalues, determine the dimensionality of the data you want to keep (i.e., how many principal components you want to keep), which accounts for most of the variance. Explain your reason."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "(f) Display the top-5 leading eigenvectors (corresponding to the top-5 largest eigenvalues) in 5 figures.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "(g) Display, respectively, the reconstructed 100th images using 10, 100, 200, and 399 principal components. (Hint: In Lecture 9 (page 19), we have learned that $x = v v^\\top x$ if we project $x$ into 1-dimensional space using the 1st principal component. Reconstructed $\\hat{x}$ using top-$K$ principal components is a straightforward extension: $\\hat{x} = \\sum_{k=1}^{K} v_k v_k^\\top x$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
